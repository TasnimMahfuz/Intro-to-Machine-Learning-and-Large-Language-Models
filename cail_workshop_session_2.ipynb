{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Input Embeddings:\n",
        "\n",
        "The `InputEmbeddings` class is responsible for converting tokenized input sequences into dense vector representations (embeddings) suitable for the Transformer model. This embedding process includes scaling the embeddings by the square root of the model’s dimension (`d_model`), a standard technique introduced in the original Transformer paper. This class uses PyTorch’s `nn.Embedding` to transform indices into corresponding vectors. The `forward` method processes the input batch and returns the embeddings.\n"
      ],
      "metadata": {
        "id": "oW1P09jeJ3x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch  # Core library for tensor operations\n",
        "import torch.nn as nn  # PyTorch's neural network module\n",
        "import math  # Library to perform mathematical operations, used for sqrt\n",
        "import numpy as np # Numpy library\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Dimension of the embeddings (model size)\n",
        "        self.vocab_size = vocab_size  # Vocabulary size, i.e., the number of unique tokens\n",
        "        # Embedding layer that converts token indices to vectors of size d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a batch of input token indices of shape (batch_size, seq_len)\n",
        "        # We apply the embedding layer to convert token indices to vectors\n",
        "        # Shape becomes (batch_size, seq_len, d_model)\n",
        "\n",
        "        # The embeddings are scaled by sqrt(d_model), as suggested in the Transformer paper\n",
        "        # You will need to use the math library to apply the scaling factor.\n",
        "\n",
        "        # TODO: Complete the return statement by multiplying the embeddings by the square root of d_model\n",
        "        return self.embedding(x) *___   # <--- fill with the necessary code\n"
      ],
      "metadata": {
        "id": "TxhejtuKKI10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading Input Embeddings\n",
        "Do not move to the next section without succeeding this section"
      ],
      "metadata": {
        "id": "MXec1nB9Kbo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function to check if sqrt(d_model) scaling is correctly implemented\n",
        "def validate_input_embeddings():\n",
        "    # Define test parameters\n",
        "    vocab_size = 1000  # Assume a vocabulary size of 1000 words\n",
        "    d_model = 512  # Assume the model size (embedding dimension) is 512\n",
        "\n",
        "    # Create the InputEmbeddings instance\n",
        "    input_embed = InputEmbeddings(d_model, vocab_size)\n",
        "\n",
        "    # Generate random token indices for a batch of sequences\n",
        "    batch_size = 4\n",
        "    seq_len = 10  # Example sequence length\n",
        "    sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    # Get embeddings for the input batch\n",
        "    embeddings = input_embed(sample_input)\n",
        "\n",
        "    # Check if the embeddings have been scaled by sqrt(d_model)\n",
        "    expected_scale = math.sqrt(d_model)  # Correct scaling factor\n",
        "\n",
        "    # Get the norm of the first token embedding before and after scaling\n",
        "    original_embedding = input_embed.embedding(sample_input)[0, 0]  # First token embedding without scaling\n",
        "    scaled_embedding = embeddings[0, 0]  # First token embedding with scaling\n",
        "\n",
        "    # Calculate the norm ratio to check if the scaling was applied\n",
        "    scaling_ratio = scaled_embedding.norm().item() / original_embedding.norm().item()\n",
        "\n",
        "    # Check if the scaling ratio is close to the expected sqrt(d_model)\n",
        "    if math.isclose(scaling_ratio, expected_scale, rel_tol=1e-5):\n",
        "        print(f\"Test passed!✅ The embeddings were correctly scaled by sqrt(d_model) = {expected_scale:.2f}\")\n",
        "    else:\n",
        "        print(f\"Test failed.❌ Expected scaling factor: {expected_scale:.2f}, but got {scaling_ratio:.2f}. Please check your code.\")\n",
        "\n",
        "# Run the validation function\n",
        "validate_input_embeddings()\n"
      ],
      "metadata": {
        "id": "S10LI8kCKic1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Positional Encoding\n",
        "\n",
        "The `PositionalEncoding` class generates positional encodings that help the Transformer model recognize the order of tokens in a sequence, as it lacks any inherent notion of token position. This encoding is added to the input embeddings, with sine and cosine functions applied to the positions in alternating dimensions of the encoding matrix. You will complete the code where necessary and ensure the positional encodings are applied correctly.\n",
        "\n",
        "![CodeCogsEqn.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAY0AAABHBAMAAAAJjQaEAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAMu/Nq0Tdu4mZEHZUImZgocIvAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAKHklEQVRoBe1afWxbVxX/ObZj+/kjlqhARYKaik1iQBOt2wTTtLhqoJABMeNTgIgLUtkqlURTQV2HFiOKShEjYaq0ARt5Kt2mTZNs0DYBAmI2qEBFjRETK/8sT+u0oYIWN+0+mjZ9/M691/G3Y7cbtcWO5HvOPef37rvn3XvPve88A91I666xHx4ZlZ6t3/0RYf5t2zLCe4wmB7aM4m4bCF0B3052fr+NdT3mA7vrz/WdARYywNUFWBT9CWCq9/zw2dllYCCNwMvs/ISNQFq502ue+DA+CWSLWCTDcAreC73mgunvtANsKGIwzzqLoPvqb4ylt9hwAZhL+F322nLjwJi7lOotD1RvrSWyqZngabKIOBN93k2Q9Rr5GaPgxn1nyQJSAA+uKNZbRZDrOnIGoXPs9sIyAlcBYUawniPvKaB/GWHxY8zBPH0IDvWcFwy5HI/ZPKx3A7HXgE9xjR/iar9YeqCDC9/RAdZAraYhaH7Ejp0naj6FXzpcIil4vt60fU+hqUkbrOQagEqzb63WKsFaPlyvMprZkyO325SjI9vUvvHYwdtbNP9xfZVn0HU3Dp7+kKrdvFHTe1gL2BrQVmntbQtWCWp+xUSLXle2oOWHbKMbTvMoNjWpav2vkFlHJNjdpxStiqe+cqcyP5cHPtcKqG0CK1OoqlbW8/ayfbRPft1znsQyvMir43Qf1xVbei/gGRKpFQUda3GXACZygC/fCqpsAivTW8pijcSQ2xF9TKMtN0UhLPsmw7UcMQGavKJtSd/lMMrw4cv8RdMitSSBlenaslgjhRiuOqF/aLDa+bn/q34vMlaTfgR8Vlubl9YMbWOZEuCGktAe96eb4vr0s2xqrzV480oTkmMMxyMubGxIStwP3KSEFkVMHB9PlBAvloT2uC/XDOfd6L6vma2hPjap1P1qSQT04hrMANw3kgD31Nakhp/HBkNZuyS1xQfibcHaAVm6D1k1jAvKm6jMrsflYn9irSZ8Mo7y5qap3ylJbfGb20K1B9KbpBxfeEwekjLi2oC82SMog+XZOmodHKEKnntGMmT49O49woSs37HYkEBwqyPVUFLKRqSzHQrm2TqDx7b9hKj9jZAXqTuorptNkwVfVXKI8edpqcM7wyKAM+vsEI+buDuFD9uE7YUnyeoqzSXx7diKVMPpVWWNsF9lOxTscPTcMynIvT5YA7qU6hPq4gn2+NkdKSX3c2t3xQOoafJFDCYR5Yzzvp9TKAlwNgSKCmmKiYI/FzwvFb/RW48Ysg1EZugUkyAC+4X1yl6+ctPU/MBkLuuAjQvWcr+6efMe7Qay52GN50U9IOWoJfOMYzRFO0/S+KZ2hyZNPFoHIMdszjw1KsCiWyJbYwJpyXZo2KTHLdAP/pTvGnDJ5Qa2B7+0XCJZK15bagMpFkk/p4DlIiYzwct4MOxQqKANGYSwkBZNtFmA09kOBbNmwhJOBvmT4DJQ8vgSOJvBQpxFsPI0M5uWRQGLj11skDczj4s+eXpSTLt32KIv0a0iTOekjErPGlEp26FgkpKKcoAt8fp180OeubfyNCNrJaxcUT7Cy8kSXsJ4kcAsb+1z3Ssq+upLSmXQltJq5kcp26FgA2xJxqTp6ElTnVJW/BjgIy+R5eaVeJRjJTZkE3TqLKaHKM+zC3h2rHL4DlADmXuk5j3T2Q4N28CWZD01HT3VVoeFeubjZn3KtWr7IL+eHuRFMZ6ko6cwNkN5SgpYFS8HYaXxyaRrsc5pk2yHhs1l+IgSbEZfRNvrQGqdTyfKLYX0w/Yn2XtH1NMs55OYzfEJMnTdZjM/xp+hv5CflDFLUvAzRAitxt1HdL2U7dCwqThbyPgdVMwCjbuEctzmxcPJcgv9eq0cSjHKZkQ9zNtO2JijH74Leri+swr3C2QG8zMeEWIJFg1IQqBkOzRM3g4mCiEbtzSArqkKH/gWH1i6FvecKFynrJZEBZ/odSzUErZoDHEGHEpyX8jAfy2N76JxKs8Cx6VIYjrlsymog4xoaqiU7VAwtUiW8DAwUgNrq3oXsg6ifMrVJGecsFnayjIn0TA6LXmjWJpF5MwoFvPkLyO8ifVvAG91ZIolWLGu3LFjBwPcfPw/rCGQkbKeStkOBVMpqc0YBV6oR66t2Y5QugFqD/Dkxo1Lk8YUkYwDzyXuVVREZbqHLhxWhzo8PXKHzXr4o9vuJ8NtO1kwBpNyCO52RKc2ThFqyWQ7FCzM8w3+PlLgYrdrcW3UNyFYbABrGvEFewN//YwzDen79VoevTohb74T9Co2MOSR6FJFcoBrTi/SxBjTmDL16nvrVa005iWuFaSRbSH+aF+8xhDK1Ciqqn22CrlVutVKclVaFWQydkKd4nXbNyK1UHsX6Wlzkk1u1mlsj+Tr9OFMnaq14tetzY2tQQc4UGv6Va2iur6LJ6dUtapUe6gklPmDZbE9qb/QHq4K9RJrEvgryUpX1urlZ3D14FK+Xk/NXfXaLfWq1proZGt7I6s/DyeSrnm4vngjaFkXyZTlNaWYsyakFvDnWsXa9fWf+dJMMONUA2WMWtKfWlqrje+srrZTi9Q81zau4d424/mBXYX0OFXVNytvPoH/ryfQMgnRW4/ibc3Oeb3lRhvJ5svuEL8Y8SXh3n1OmeGf3/uEaA2jFE5KvYvpgSdUvuGIrfLWhkV2qpdLw6T3AYdFF5PnmmPih6SMvJkSU+nBD/BNi+cNMqGBgubdW6rv/b4hvkcvwzBsZXcX7RLjrPrtk93rgOmZ8iOb4Zicg2HWa7Qt5AyjfOfFJSXMHf43TP//wuHNTmNes4gkbQaGDONITdZ/q9SBOHgrkd1Byo9pOTEOMoGkWFDSBn1Fwyg6da8cpUAsOYTuIOXHWJydmbAN80nyqX/ZME6xgqdY21kTiCWn0x2k/JBkKbOMhgWUHyuG8RMkwpMyUpUUcKTmT0vZFaT8kOwrU7CGqc9b3lOGcV7h57WvTiYQB2e6wgfpRBt++O/7w8/sf598acujf6S/R/8V14H42FF4nW70o6DHo6AHYsUw3dPoJzfhqXxkEqFcNKECsS/nT3XR9lgejwntx4T2o1/7UUpoRmJpZoS5uk/wjxYqEL+AYKHDFOYbOXh6ncsynrDlozCZ+ld7/7Jh5ubMGd6EQA6b+GVRBeKvHXsc+97InnXWtvJjOs+LXBimPscPFA0zzfWluIlkC9EVxIoSiC15I7nRGLuAKT/mc+wJ93PNIvLZgvu5ZqaPC4gWccLPncSbk0D8hSINK38z1svPlB/ZGX2+0sw6y27xfKWZ6eIJOZ78NIbteF4H4u2IvL3oXH4HTA+UH4E0D7UrMEz+NIG5Qolp4O8RzOPzeRw/5kAF4uN/PYkfpkwrl5/F5FAoH6H5PdIw9cn7Fg4Je0nWGxSSQzqOFHBPmUV2ITTJj3ma9YIf1nWD7pXX89v7j/c57K9hWH+vej83rB1H/guyjLyTqJKwtAAAAABJRU5ErkJggg==)\n",
        "\n",
        "![CodeCogsEqn (1).png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "M7pK-YaZNFpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Class to create positional encodings for sequence data\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Model dimensionality\n",
        "        self.seq_len = seq_len  # Length of the input sequences\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer to prevent overfitting\n",
        "\n",
        "        # Create a matrix to hold positional encodings of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        # Create a vector of shape (seq_len, 1) to represent positions\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # Shape: (seq_len, 1)\n",
        "\n",
        "        # Create a vector of shape (d_model / 2) for frequency terms\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # TODO: Apply sine to even indices in the positional encoding matrix\n",
        "        pe[:, 0::2] = torch.sin(position * ____)  # <--- complete this part\n",
        "\n",
        "        # TODO: Apply cosine to odd indices in the positional encoding matrix\n",
        "        pe[:, 1::2] = ___  # <--- complete this part\n",
        "\n",
        "        # Add a batch dimension to the positional encoding matrix\n",
        "        pe = pe.unsqueeze(0)  # Shape becomes (1, seq_len, d_model)\n",
        "\n",
        "        # Register the positional encoding as a buffer so it's not updated during backpropagation\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "\n",
        "        # Apply dropout to prevent overfitting\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "zwvHIQpeNRvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading Positional Encoding\n",
        "Do not move to the next section without succeeding this section"
      ],
      "metadata": {
        "id": "LC5vEaisNVTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for PositionalEncoding class\n",
        "\n",
        "def validate_positional_encoding():\n",
        "    # Define test parameters\n",
        "    d_model = 512  # Model size (embedding dimension)\n",
        "    seq_len = 50   # Sequence length\n",
        "    dropout = 0.1  # Dropout rate\n",
        "\n",
        "    # Create the PositionalEncoding instance\n",
        "    positional_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
        "\n",
        "    # Create a batch of zero vectors representing input embeddings\n",
        "    batch_size = 4\n",
        "    input_embeddings = torch.zeros(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Apply positional encoding to the input embeddings\n",
        "    encoded_embeddings = positional_encoding(input_embeddings)\n",
        "\n",
        "    # Test 1: Check if positional encodings have been added (not all zeros)\n",
        "    test1_passed = not torch.allclose(encoded_embeddings, torch.zeros_like(encoded_embeddings))\n",
        "\n",
        "    # Test 2: Check shape of the output\n",
        "    test2_passed = encoded_embeddings.shape == (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Test 3: Validate that the positional encoding for position 0 is not all zeros\n",
        "    position_0_encoding = positional_encoding.pe[0, 0, :].detach().numpy()\n",
        "    test3_passed = not np.all(position_0_encoding == 0)\n",
        "\n",
        "    # Test 4: Check that applying positional encodings to different input embeddings yields different results\n",
        "    input_embeddings_2 = torch.ones(batch_size, seq_len, d_model)  # A different input\n",
        "    encoded_embeddings_2 = positional_encoding(input_embeddings_2)\n",
        "    test4_passed = not torch.allclose(encoded_embeddings, encoded_embeddings_2)\n",
        "\n",
        "    # Test 5: Check if dropout keeps some elements zero (this might fail sometimes depending on random drop)\n",
        "    # To make this deterministic, we can set a seed (optional)\n",
        "    torch.manual_seed(0)\n",
        "    encoded_embeddings_dropped = positional_encoding(input_embeddings)\n",
        "    # Apply the same dropout manually to see if any values are zero\n",
        "    dropped_check = torch.sum(encoded_embeddings_dropped == 0).item() > 0\n",
        "    test5_passed = dropped_check\n",
        "\n",
        "    # Summarize results\n",
        "    tests = [test1_passed, test2_passed, test3_passed, test4_passed, test5_passed]\n",
        "    print(f\"Results of PositionalEncoding Validation: {sum(tests)}/{len(tests)} tests passed.\")\n",
        "\n",
        "    # Visualize the positional encoding for a sample sequence\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Extract the positional encodings (before dropout) for visualization\n",
        "    pe = positional_encoding.pe.squeeze(0).detach().numpy()  # Shape: (seq_len, d_model)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pe, cmap=\"coolwarm\", cbar=True)\n",
        "    plt.title('Heatmap of Positional Encodings (Sequence Length vs Embedding Dimensions)')\n",
        "    plt.xlabel('Embedding Dimension')\n",
        "    plt.ylabel('Position in Sequence')\n",
        "    plt.show()\n",
        "\n",
        "# Call the validation function\n",
        "validate_positional_encoding()\n"
      ],
      "metadata": {
        "id": "4vX8SVQ0NaJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Layer Normalization\n",
        "\n",
        "`LayerNormalization` is a custom implementation of layer normalization, which normalizes the input along the last dimension (features) for each sequence in the batch. It helps to stabilize and speed up training by ensuring that each feature in the input has zero mean and unit variance. You will complete the code where necessary to implement the normalization and broadcasting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z98itprjkzcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Custom implementation of Layer Normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps  # Small value to prevent division by zero\n",
        "        # Learnable parameters for scaling (alpha) and bias\n",
        "        self.alpha = nn.Parameter(torch.ones(features))  # Alpha is initialized to 1\n",
        "        self.bias = nn.Parameter(torch.zeros(features))  # Bias is initialized to 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "\n",
        "        # TODO: Calculate the mean of x along the last dimension, keeping the dimension for broadcasting\n",
        "        mean = ___  # <--- Fill up this section (x.mean(dim=-1, keepdim=True))\n",
        "\n",
        "        # TODO: Calculate the standard deviation of x along the last dimension, keeping the dimension for broadcasting\n",
        "        std = ___  # <--- Fill up this section (x.std(dim=-1, keepdim=True))\n",
        "\n",
        "        # TODO: Normalize x using the calculated mean and standard deviation, and scale with alpha, bias\n",
        "        return ___  # <--- Fill up this section (self.alpha * (x - mean) / (std + self.eps) + self.bias)\n"
      ],
      "metadata": {
        "id": "saOJv8FwOj2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading Layer Normalization\n",
        "Do not move to the next section without succeeding this section"
      ],
      "metadata": {
        "id": "YCVGEvCisss2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for LayerNormalization class\n",
        "\n",
        "def validate_layer_normalization():\n",
        "    # Define test parameters\n",
        "    features = 512  # Hidden size\n",
        "    batch_size = 4  # Batch size\n",
        "    seq_len = 10    # Sequence length\n",
        "    eps = 1e-6      # Small value for numerical stability\n",
        "\n",
        "    # Create LayerNormalization instance\n",
        "    layer_norm = LayerNormalization(features, eps)\n",
        "\n",
        "    # Create random input tensor with shape (batch_size, seq_len, features)\n",
        "    input_tensor = torch.randn(batch_size, seq_len, features)\n",
        "\n",
        "    # Apply layer normalization\n",
        "    normalized_output = layer_norm(input_tensor)\n",
        "\n",
        "    # Test 1: Check that the output has the same shape as the input\n",
        "    test1_passed = normalized_output.shape == input_tensor.shape\n",
        "\n",
        "    # Test 2: Verify that the mean of the output is approximately zero for each feature across sequences\n",
        "    mean_per_feature = normalized_output.mean(dim=-1)  # Mean along the feature dimension\n",
        "    test2_passed = torch.allclose(mean_per_feature, torch.zeros_like(mean_per_feature), atol=1e-5)\n",
        "\n",
        "    # Test 3: Verify that the standard deviation of the output is approximately one for each feature across sequences\n",
        "    std_per_feature = normalized_output.std(dim=-1)  # Std along the feature dimension\n",
        "    test3_passed = torch.allclose(std_per_feature, torch.ones_like(std_per_feature), atol=1e-5)\n",
        "\n",
        "    # Test 4: Apply layer normalization to different inputs and verify that they give different results\n",
        "    input_tensor_2 = torch.randn(batch_size, seq_len, features)\n",
        "    normalized_output_2 = layer_norm(input_tensor_2)\n",
        "    test4_passed = not torch.allclose(normalized_output, normalized_output_2)\n",
        "\n",
        "    # Summarize results\n",
        "    tests = [test1_passed, test2_passed, test3_passed, test4_passed]\n",
        "    print(f\"Results of LayerNormalization Validation: {sum(tests)}/{len(tests)} tests passed.\")\n",
        "\n",
        "\n",
        "\n",
        "# Call the validation function\n",
        "validate_layer_normalization()\n"
      ],
      "metadata": {
        "id": "R6V5roUoszdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feed Forward Block\n",
        "The FeedForwardBlock is a crucial component in the Transformer architecture. It applies a two-layer feed-forward network to each position in the sequence, separately and identically. The first layer maps from the model dimension (`d_model`) to a higher-dimensional space (`d_ff`), followed by a ReLU activation function, dropout for regularization, and a second linear layer that projects back to the model dimension. This block allows the model to capture more complex transformations after the multi-head attention mechanism.\n",
        "\n",
        "Fill up this section to complete the code and implement the feed-forward block."
      ],
      "metadata": {
        "id": "eatF-xpMs9zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # TODO what module ?\n",
        "        self.linear_2 = nn.____(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        # TODO what activation?\n",
        "        return self.linear_2(self.dropout(torch.____(self.linear_1(x))))\n",
        "\n"
      ],
      "metadata": {
        "id": "O2LKfwD_tA4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Feed Forward Network\n",
        "Check with answers to see if it matches\n"
      ],
      "metadata": {
        "id": "N8HTZIt_uMsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "mGL8cymVuP4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Residual Connection\n",
        "The `ResidualConnection` class implements a key part of the Transformer architecture, which adds a residual (or skip) connection around a sublayer, such as a feed-forward network or multi-head attention. Additionally, layer normalization and dropout are applied to stabilize training and improve generalization. Residual connections are essential for deep models to mitigate the vanishing gradient problem.\n",
        "\n",
        "Fill up this section by adding the key components: layer normalization, dropout, and the residual addition.\n",
        "\n"
      ],
      "metadata": {
        "id": "gcGIXdQH_FDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)  # Initialize dropout layer with the specified dropout rate\n",
        "        self.norm = LayerNormalization(features)  # Initialize layer normalization with the given number of features\n",
        "\n",
        "    # Apply layer normalization to the input x\n",
        "    # Pass the normalized output through the sublayer (e.g., an attention layer or feed-forward network)\n",
        "    # Apply dropout to the result of the sublayer\n",
        "    # Add the original input x to the result to create a residual connection\n",
        "    # TODO complete the forward method\n",
        "    def forward(self, x, sublayer):\n",
        "        return ____  # Implement the forward pass as\n"
      ],
      "metadata": {
        "id": "ij2clRM__LDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Residual Connection\n",
        "Match your output with the following *section*"
      ],
      "metadata": {
        "id": "I_DlDSBrDOv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THIS. JUST CHECK\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)  # Initialize dropout layer with the specified dropout rate\n",
        "        self.norm = LayerNormalization(features)  # Initialize layer normalization with the given number of features\n",
        "\n",
        "    # Apply layer normalization to the input x\n",
        "    # Pass the normalized output through the sublayer (e.g., an attention layer or feed-forward network)\n",
        "    # Apply dropout to the result of the sublayer\n",
        "    # Add the original input x to the result to create a residual connection\n",
        "    # TODO complete the forward method\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "7Ot8O-7-DW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Multi-Head Attention Block\n",
        "In this section, we will implement a Multi-Head Attention mechanism, which is a crucial component of the Transformer architecture. Multi-Head Attention allows the model to focus on different parts of the input sequence simultaneously. The mechanism involves projecting the input into multiple heads, calculating attention scores, and then combining the results.\n",
        "\n",
        "The following code defines the `MultiHeadAttentionBlock` class, which includes the initialization of necessary weights and the forward pass computation."
      ],
      "metadata": {
        "id": "JwFlq_ekEtFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "\n",
        "        # TODO define self.w_k\n",
        "        self.w_k = ____\n",
        "\n",
        "        # TODO self.w_v\n",
        "        self.w_v = ____\n",
        "\n",
        "        # TODO self.w_o\n",
        "        self.w_o = ____\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "\n",
        "        # TODO completion the attention score formula\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / ____\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        # TODO complete the value transformation\n",
        "        value = ____\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "v_s1nanNEy76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Multi-Head Attention\n",
        "Check the answers and match with yours\n"
      ],
      "metadata": {
        "id": "aT0WvsGuGBAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THIS. JUST CHECK THE CODE\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "Zm9ktAJnGHgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Encoder\n",
        "The Encoder class in a Transformer architecture consists of multiple stacked Encoder Blocks. Each block applies self-attention and a feed-forward network, with residual connections enhancing the model's capacity to learn complex representations. The final output is normalized to improve convergence during training.\n",
        "\n",
        "In this section, we will implement the `Encoder` class, which utilizes a list of `EncoderBlock` instances.\n"
      ],
      "metadata": {
        "id": "DqYC_c3kGoHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # TODO What are the three inputs to the residual connection?\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(____, ____, ____, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "Qx7qeMc3GryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Encoder\n",
        "Check the answers to match with yours"
      ],
      "metadata": {
        "id": "eI1dtoLoRUVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "gZgkb2SMRZvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Decoder\n",
        "The Decoder class in a Transformer architecture is responsible for generating outputs based on encoded inputs. It consists of multiple stacked Decoder Blocks, each incorporating self-attention, cross-attention to encoder outputs, and feed-forward networks, complemented by residual connections to enhance learning.\n",
        "\n",
        "In this section, we will implement the `Decoder` class along with its `DecoderBlock`, which processes the input sequence and attends to the encoder's output.\n"
      ],
      "metadata": {
        "id": "a9biH1cARhvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        # TODO complete the residual connections\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(____)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        # TODO finish initializing the Decoer\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "yq4pnPyoR74k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Decoder\n",
        "Check the answers to match with yours"
      ],
      "metadata": {
        "id": "GI2gsLSKSaly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "447psi0QSgdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Projection Layer\n",
        "The Projection Layer is an essential component of the Transformer architecture, primarily used to map the output of the decoder (or any other component with a d_model-sized output) to the vocabulary size. This layer facilitates the generation of probabilities for each token in the vocabulary for the next predicted token in a sequence.\n",
        "\n",
        "In this section, we will implement the `ProjectionLayer`, which will transform the model's output to match the vocabulary size.\n"
      ],
      "metadata": {
        "id": "FHUgStpmSyia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        #TODO what module should be here?\n",
        "        self.proj = nn.____(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "zznCahVgS08C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Projection Layer\n",
        "Check the answers to match with yours"
      ],
      "metadata": {
        "id": "wPjRj84vS5wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "q5GvUwcfTBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Transformer Class\n",
        "The `Transformer` class encapsulates the entire architecture of the Transformer model. It integrates the encoder and decoder components along with their respective input embeddings, positional encodings, and the projection layer. This class handles the encoding of the source sequences and the decoding of target sequences while managing the flow of information throughout the architecture.\n",
        "\n",
        "In this section, we will implement the `Transformer` class and its key functionalities, including encoding and decoding.\n"
      ],
      "metadata": {
        "id": "4CB_Wqu0cpu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        # TODO similarly, fill up for the rest of the attributes\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        #TODO complete the decode method\n",
        "        # what is tgt going to be here?\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "1009oUAzcsmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-grading Transformer class\n",
        "Check with the answers to match with yours"
      ],
      "metadata": {
        "id": "gv_RraBhdIfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "5Wat-HCHdNfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Putting It All Together\n",
        "In this segment, we will finally finish building our transformer model"
      ],
      "metadata": {
        "id": "ybssDiWZdRIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    # TODO Fill this up\n",
        "    tgt_embed = ____\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    # TODO fill this upo\n",
        "    src_pos = ____\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        # TODO  complete the decoder block\n",
        "        decoder_self_attention_block = ____\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "-8am3T3gdZpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Self-Grading\n",
        "Check with the answers to match with yours"
      ],
      "metadata": {
        "id": "bcYotHQndxkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "ZRQFXJbod2fW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}